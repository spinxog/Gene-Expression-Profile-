{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from ndlinear import NdLinear\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import autoaugment, transforms\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    autoaugment.AutoAugment(autoaugment.AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=True,\n",
    "    download=True, \n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# For validation set, we should use the test transforms (no augmentation)\n",
    "val_dataset.dataset.transform = test_transform\n",
    "\n",
    "# Test dataset\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=True, \n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=False, \n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=128, \n",
    "    shuffle=False, \n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EnhancedNdLinearCNN(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.4):\n",
    "        super(EnhancedNdLinearCNN, self).__init__()\n",
    "        \n",
    "        # First block - keep input channels small but increase gradually\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Second block - increase channels\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Third block - further increase channels\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Fourth block - deep features\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # NdLinear layers with proper dimensions\n",
    "        self.ndlinear1 = NdLinear(input_dims=(512,), hidden_size=(256,))\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.ndlinear2 = NdLinear(input_dims=(256,), hidden_size=(128,))\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.ndlinear3 = NdLinear(input_dims=(128,), hidden_size=(10,))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.ndlinear1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.ndlinear2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.ndlinear3(x)\n",
    "        return x\n",
    "    \n",
    "model = EnhancedNdLinearCNN()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Loss and optimizer with stronger regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "# Cosine annealing scheduler often works better than ReduceLROnPlateau\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Batch: 0 | Loss: 2.374 | Acc: 10.94%\n",
      "Epoch: 0 | Batch: 100 | Loss: 1.873 | Acc: 28.89%\n",
      "Epoch: 0 | Batch: 200 | Loss: 1.691 | Acc: 36.66%\n",
      "Epoch: 0 | Batch: 300 | Loss: 1.575 | Acc: 41.64%\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Current learning rate: 0.001\n",
      "Validation Loss: 1.159 | Validation Acc: 58.76%\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Model saved with accuracy: 58.76%\n",
      "Epoch 1/200 completed\n",
      "Best accuracy so far: 58.76%\n",
      "Epoch: 1 | Batch: 0 | Loss: 1.257 | Acc: 53.91%\n",
      "Epoch: 1 | Batch: 100 | Loss: 1.154 | Acc: 59.18%\n",
      "Epoch: 1 | Batch: 200 | Loss: 1.111 | Acc: 60.84%\n",
      "Epoch: 1 | Batch: 300 | Loss: 1.080 | Acc: 62.08%\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Current learning rate: 0.0009999383162408303\n",
      "Validation Loss: 0.859 | Validation Acc: 69.90%\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Model saved with accuracy: 69.90%\n",
      "Epoch 2/200 completed\n",
      "Best accuracy so far: 69.90%\n",
      "Epoch: 2 | Batch: 0 | Loss: 0.988 | Acc: 62.50%\n",
      "Epoch: 2 | Batch: 100 | Loss: 0.924 | Acc: 67.57%\n",
      "Epoch: 2 | Batch: 200 | Loss: 0.908 | Acc: 68.33%\n",
      "Epoch: 2 | Batch: 300 | Loss: 0.893 | Acc: 68.83%\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Current learning rate: 0.0009997532801828658\n",
      "Validation Loss: 0.801 | Validation Acc: 71.56%\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Model saved with accuracy: 71.56%\n",
      "Epoch 3/200 completed\n",
      "Best accuracy so far: 71.56%\n",
      "Epoch: 3 | Batch: 0 | Loss: 0.788 | Acc: 74.22%\n",
      "Epoch: 3 | Batch: 100 | Loss: 0.780 | Acc: 73.56%\n",
      "Epoch: 3 | Batch: 200 | Loss: 0.772 | Acc: 73.80%\n",
      "Epoch: 3 | Batch: 300 | Loss: 0.775 | Acc: 73.59%\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Current learning rate: 0.0009994449374809851\n",
      "Validation Loss: 0.661 | Validation Acc: 75.90%\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Model saved with accuracy: 75.90%\n",
      "Epoch 4/200 completed\n",
      "Best accuracy so far: 75.90%\n",
      "Epoch: 4 | Batch: 0 | Loss: 0.781 | Acc: 67.97%\n",
      "Epoch: 4 | Batch: 100 | Loss: 0.684 | Acc: 77.04%\n",
      "Epoch: 4 | Batch: 200 | Loss: 0.688 | Acc: 76.92%\n",
      "Epoch: 4 | Batch: 300 | Loss: 0.685 | Acc: 77.04%\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Current learning rate: 0.0009990133642141358\n",
      "Validation Loss: 0.663 | Validation Acc: 76.88%\n",
      "Current learning rate: 0.000998458666866564\n",
      "Model saved with accuracy: 76.88%\n",
      "Epoch 5/200 completed\n",
      "Best accuracy so far: 76.88%\n",
      "Epoch: 5 | Batch: 0 | Loss: 0.560 | Acc: 82.03%\n",
      "Epoch: 5 | Batch: 100 | Loss: 0.638 | Acc: 78.37%\n",
      "Epoch: 5 | Batch: 200 | Loss: 0.631 | Acc: 78.80%\n",
      "Epoch: 5 | Batch: 300 | Loss: 0.625 | Acc: 79.12%\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Current learning rate: 0.000998458666866564\n",
      "Validation Loss: 0.703 | Validation Acc: 75.96%\n",
      "Current learning rate: 0.00099778098230154\n",
      "Epoch 6/200 completed\n",
      "Best accuracy so far: 76.88%\n",
      "Epoch: 6 | Batch: 0 | Loss: 0.589 | Acc: 78.12%\n",
      "Epoch: 6 | Batch: 100 | Loss: 0.573 | Acc: 80.89%\n",
      "Epoch: 6 | Batch: 200 | Loss: 0.573 | Acc: 80.91%\n",
      "Epoch: 6 | Batch: 300 | Loss: 0.574 | Acc: 80.74%\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Current learning rate: 0.00099778098230154\n",
      "Validation Loss: 0.546 | Validation Acc: 81.10%\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Model saved with accuracy: 81.10%\n",
      "Epoch 7/200 completed\n",
      "Best accuracy so far: 81.10%\n",
      "Epoch: 7 | Batch: 0 | Loss: 0.576 | Acc: 80.47%\n",
      "Epoch: 7 | Batch: 100 | Loss: 0.530 | Acc: 82.67%\n",
      "Epoch: 7 | Batch: 200 | Loss: 0.538 | Acc: 82.20%\n",
      "Epoch: 7 | Batch: 300 | Loss: 0.535 | Acc: 82.21%\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Current learning rate: 0.0009969804777275899\n",
      "Validation Loss: 0.603 | Validation Acc: 79.48%\n",
      "Current learning rate: 0.000996057350657239\n",
      "Epoch 8/200 completed\n",
      "Best accuracy so far: 81.10%\n",
      "Epoch: 8 | Batch: 0 | Loss: 0.581 | Acc: 76.56%\n",
      "Epoch: 8 | Batch: 100 | Loss: 0.499 | Acc: 83.52%\n",
      "Epoch: 8 | Batch: 200 | Loss: 0.496 | Acc: 83.61%\n",
      "Epoch: 8 | Batch: 300 | Loss: 0.502 | Acc: 83.43%\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Current learning rate: 0.000996057350657239\n",
      "Validation Loss: 0.451 | Validation Acc: 84.06%\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Model saved with accuracy: 84.06%\n",
      "Epoch 9/200 completed\n",
      "Best accuracy so far: 84.06%\n",
      "Epoch: 9 | Batch: 0 | Loss: 0.406 | Acc: 86.72%\n",
      "Epoch: 9 | Batch: 100 | Loss: 0.457 | Acc: 84.78%\n",
      "Epoch: 9 | Batch: 200 | Loss: 0.458 | Acc: 84.69%\n",
      "Epoch: 9 | Batch: 300 | Loss: 0.462 | Acc: 84.58%\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Current learning rate: 0.0009950118288582787\n",
      "Validation Loss: 0.462 | Validation Acc: 84.02%\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Epoch 10/200 completed\n",
      "Best accuracy so far: 84.06%\n",
      "Epoch: 10 | Batch: 0 | Loss: 0.493 | Acc: 85.94%\n",
      "Epoch: 10 | Batch: 100 | Loss: 0.417 | Acc: 85.84%\n",
      "Epoch: 10 | Batch: 200 | Loss: 0.424 | Acc: 85.70%\n",
      "Epoch: 10 | Batch: 300 | Loss: 0.434 | Acc: 85.45%\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Current learning rate: 0.0009938441702975688\n",
      "Validation Loss: 0.484 | Validation Acc: 83.48%\n",
      "Current learning rate: 0.000992554663077387\n",
      "Epoch 11/200 completed\n",
      "Best accuracy so far: 84.06%\n",
      "Epoch: 11 | Batch: 0 | Loss: 0.219 | Acc: 89.84%\n",
      "Epoch: 11 | Batch: 100 | Loss: 0.398 | Acc: 86.56%\n",
      "Epoch: 11 | Batch: 200 | Loss: 0.400 | Acc: 86.61%\n",
      "Epoch: 11 | Batch: 300 | Loss: 0.406 | Acc: 86.35%\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Current learning rate: 0.000992554663077387\n",
      "Validation Loss: 0.413 | Validation Acc: 85.54%\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Model saved with accuracy: 85.54%\n",
      "Epoch 12/200 completed\n",
      "Best accuracy so far: 85.54%\n",
      "Epoch: 12 | Batch: 0 | Loss: 0.455 | Acc: 81.25%\n",
      "Epoch: 12 | Batch: 100 | Loss: 0.375 | Acc: 87.41%\n",
      "Epoch: 12 | Batch: 200 | Loss: 0.376 | Acc: 87.43%\n",
      "Epoch: 12 | Batch: 300 | Loss: 0.380 | Acc: 87.24%\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Current learning rate: 0.0009911436253643444\n",
      "Validation Loss: 0.417 | Validation Acc: 85.90%\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Model saved with accuracy: 85.90%\n",
      "Epoch 13/200 completed\n",
      "Best accuracy so far: 85.90%\n",
      "Epoch: 13 | Batch: 0 | Loss: 0.377 | Acc: 89.84%\n",
      "Epoch: 13 | Batch: 100 | Loss: 0.349 | Acc: 88.22%\n",
      "Epoch: 13 | Batch: 200 | Loss: 0.354 | Acc: 88.14%\n",
      "Epoch: 13 | Batch: 300 | Loss: 0.356 | Acc: 88.11%\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Current learning rate: 0.0009896114053108828\n",
      "Validation Loss: 0.430 | Validation Acc: 85.56%\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Epoch 14/200 completed\n",
      "Best accuracy so far: 85.90%\n",
      "Epoch: 14 | Batch: 0 | Loss: 0.279 | Acc: 91.41%\n",
      "Epoch: 14 | Batch: 100 | Loss: 0.325 | Acc: 89.43%\n",
      "Epoch: 14 | Batch: 200 | Loss: 0.332 | Acc: 88.81%\n",
      "Epoch: 14 | Batch: 300 | Loss: 0.338 | Acc: 88.54%\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Current learning rate: 0.0009879583809693736\n",
      "Validation Loss: 0.431 | Validation Acc: 85.26%\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Epoch 15/200 completed\n",
      "Best accuracy so far: 85.90%\n",
      "Epoch: 15 | Batch: 0 | Loss: 0.358 | Acc: 87.50%\n",
      "Epoch: 15 | Batch: 100 | Loss: 0.293 | Acc: 90.06%\n",
      "Epoch: 15 | Batch: 200 | Loss: 0.307 | Acc: 89.50%\n",
      "Epoch: 15 | Batch: 300 | Loss: 0.311 | Acc: 89.44%\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Current learning rate: 0.0009861849601988382\n",
      "Validation Loss: 0.422 | Validation Acc: 86.18%\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Model saved with accuracy: 86.18%\n",
      "Epoch 16/200 completed\n",
      "Best accuracy so far: 86.18%\n",
      "Epoch: 16 | Batch: 0 | Loss: 0.201 | Acc: 93.75%\n",
      "Epoch: 16 | Batch: 100 | Loss: 0.283 | Acc: 90.30%\n",
      "Epoch: 16 | Batch: 200 | Loss: 0.291 | Acc: 90.19%\n",
      "Epoch: 16 | Batch: 300 | Loss: 0.298 | Acc: 90.04%\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Current learning rate: 0.0009842915805643154\n",
      "Validation Loss: 0.381 | Validation Acc: 86.94%\n",
      "Current learning rate: 0.000982278709228899\n",
      "Model saved with accuracy: 86.94%\n",
      "Epoch 17/200 completed\n",
      "Best accuracy so far: 86.94%\n",
      "Epoch: 17 | Batch: 0 | Loss: 0.261 | Acc: 91.41%\n",
      "Epoch: 17 | Batch: 100 | Loss: 0.272 | Acc: 91.10%\n",
      "Epoch: 17 | Batch: 200 | Loss: 0.281 | Acc: 90.61%\n",
      "Epoch: 17 | Batch: 300 | Loss: 0.284 | Acc: 90.50%\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Current learning rate: 0.000982278709228899\n",
      "Validation Loss: 0.395 | Validation Acc: 87.52%\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Model saved with accuracy: 87.52%\n",
      "Epoch 18/200 completed\n",
      "Best accuracy so far: 87.52%\n",
      "Epoch: 18 | Batch: 0 | Loss: 0.118 | Acc: 98.44%\n",
      "Epoch: 18 | Batch: 100 | Loss: 0.257 | Acc: 91.22%\n",
      "Epoch: 18 | Batch: 200 | Loss: 0.255 | Acc: 91.37%\n",
      "Epoch: 18 | Batch: 300 | Loss: 0.260 | Acc: 91.18%\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Current learning rate: 0.0009801468428384714\n",
      "Validation Loss: 0.366 | Validation Acc: 87.76%\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Model saved with accuracy: 87.76%\n",
      "Epoch 19/200 completed\n",
      "Best accuracy so far: 87.76%\n",
      "Epoch: 19 | Batch: 0 | Loss: 0.211 | Acc: 92.19%\n",
      "Epoch: 19 | Batch: 100 | Loss: 0.239 | Acc: 91.98%\n",
      "Epoch: 19 | Batch: 200 | Loss: 0.245 | Acc: 91.93%\n",
      "Epoch: 19 | Batch: 300 | Loss: 0.252 | Acc: 91.64%\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Current learning rate: 0.0009778965073991648\n",
      "Validation Loss: 0.386 | Validation Acc: 87.32%\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Epoch 20/200 completed\n",
      "Best accuracy so far: 87.76%\n",
      "Epoch: 20 | Batch: 0 | Loss: 0.295 | Acc: 89.06%\n",
      "Epoch: 20 | Batch: 100 | Loss: 0.216 | Acc: 92.84%\n",
      "Epoch: 20 | Batch: 200 | Loss: 0.230 | Acc: 92.21%\n",
      "Epoch: 20 | Batch: 300 | Loss: 0.235 | Acc: 92.05%\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Current learning rate: 0.0009755282581475766\n",
      "Validation Loss: 0.380 | Validation Acc: 87.34%\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Epoch 21/200 completed\n",
      "Best accuracy so far: 87.76%\n",
      "Epoch: 21 | Batch: 0 | Loss: 0.209 | Acc: 92.97%\n",
      "Epoch: 21 | Batch: 100 | Loss: 0.215 | Acc: 92.57%\n",
      "Epoch: 21 | Batch: 200 | Loss: 0.217 | Acc: 92.60%\n",
      "Epoch: 21 | Batch: 300 | Loss: 0.223 | Acc: 92.44%\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Current learning rate: 0.0009730426794137723\n",
      "Validation Loss: 0.384 | Validation Acc: 87.48%\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Epoch 22/200 completed\n",
      "Best accuracy so far: 87.76%\n",
      "Epoch: 22 | Batch: 0 | Loss: 0.313 | Acc: 92.19%\n",
      "Epoch: 22 | Batch: 100 | Loss: 0.199 | Acc: 93.22%\n",
      "Epoch: 22 | Batch: 200 | Loss: 0.198 | Acc: 93.30%\n",
      "Epoch: 22 | Batch: 300 | Loss: 0.206 | Acc: 93.09%\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Current learning rate: 0.0009704403844771124\n",
      "Validation Loss: 0.384 | Validation Acc: 88.26%\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Model saved with accuracy: 88.26%\n",
      "Epoch 23/200 completed\n",
      "Best accuracy so far: 88.26%\n",
      "Epoch: 23 | Batch: 0 | Loss: 0.176 | Acc: 95.31%\n",
      "Epoch: 23 | Batch: 100 | Loss: 0.187 | Acc: 93.64%\n",
      "Epoch: 23 | Batch: 200 | Loss: 0.193 | Acc: 93.40%\n",
      "Epoch: 23 | Batch: 300 | Loss: 0.200 | Acc: 93.31%\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Current learning rate: 0.0009677220154149334\n",
      "Validation Loss: 0.377 | Validation Acc: 88.24%\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Epoch 24/200 completed\n",
      "Best accuracy so far: 88.26%\n",
      "Epoch: 24 | Batch: 0 | Loss: 0.096 | Acc: 95.31%\n",
      "Epoch: 24 | Batch: 100 | Loss: 0.188 | Acc: 93.87%\n",
      "Epoch: 24 | Batch: 200 | Loss: 0.192 | Acc: 93.58%\n",
      "Epoch: 24 | Batch: 300 | Loss: 0.192 | Acc: 93.52%\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Current learning rate: 0.0009648882429441253\n",
      "Validation Loss: 0.384 | Validation Acc: 88.48%\n",
      "Current learning rate: 0.000961939766255643\n",
      "Model saved with accuracy: 88.48%\n",
      "Epoch 25/200 completed\n",
      "Best accuracy so far: 88.48%\n",
      "Epoch: 25 | Batch: 0 | Loss: 0.230 | Acc: 93.75%\n",
      "Epoch: 25 | Batch: 100 | Loss: 0.165 | Acc: 94.49%\n",
      "Epoch: 25 | Batch: 200 | Loss: 0.172 | Acc: 94.26%\n",
      "Epoch: 25 | Batch: 300 | Loss: 0.177 | Acc: 94.08%\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Current learning rate: 0.000961939766255643\n",
      "Validation Loss: 0.404 | Validation Acc: 87.92%\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Epoch 26/200 completed\n",
      "Best accuracy so far: 88.48%\n",
      "Epoch: 26 | Batch: 0 | Loss: 0.154 | Acc: 93.75%\n",
      "Epoch: 26 | Batch: 100 | Loss: 0.166 | Acc: 94.52%\n",
      "Epoch: 26 | Batch: 200 | Loss: 0.167 | Acc: 94.50%\n",
      "Epoch: 26 | Batch: 300 | Loss: 0.167 | Acc: 94.48%\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Current learning rate: 0.0009588773128419901\n",
      "Validation Loss: 0.392 | Validation Acc: 88.20%\n",
      "Current learning rate: 0.0009557016383177221\n",
      "Epoch 27/200 completed\n",
      "Best accuracy so far: 88.48%\n",
      "Epoch: 27 | Batch: 0 | Loss: 0.114 | Acc: 96.88%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(model, val_loader, criterion, device)\n",
      "Cell \u001b[1;32mIn[41], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, criterion, device, epoch)\u001b[0m\n\u001b[0;32m     22\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Pradip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pradip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[45], line 69\u001b[0m, in \u001b[0;36mEnhancedNdLinearCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     68\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock1(x)\n\u001b[1;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock3(x)\n\u001b[0;32m     71\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock4(x)\n",
      "File \u001b[1;32mc:\\Users\\Pradip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pradip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Pradip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Pradip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pradip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Pradip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Pradip\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop with early stopping\n",
    "num_epochs = 200\n",
    "best_acc = 0\n",
    "patience = 20\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device, epoch)\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Current learning rate: {current_lr}')\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f'Model saved with accuracy: {best_acc:.2f}%')\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        \n",
    "    # Early stopping\n",
    "    if counter >= patience:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "        break\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs} completed')\n",
    "    print(f'Best accuracy so far: {best_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# Load best model and evaluate\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "test_acc = test(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
